{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06a6d3d-7ff8-4c43-ba4f-6b373e180cb7",
   "metadata": {},
   "source": [
    "# Computing for Life Sciences 2023\n",
    "## Machine Learning Workshop - Exploring small molecule potency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599429a-e87f-4fe1-bd9c-73938e345571",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Task Overview\n",
    "- Data Exploring\n",
    "- Supervised Learning\n",
    "    - ~~Classification (Less-potent vs Potent PPI inhibitors)~~ (Lecture 1 - Alex de Sá)\n",
    "    - Regression (PPI inhibitory potency)\n",
    "        - Decision Tree\n",
    "        - Performance Evaluation\n",
    "        - Random Forest - Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a7988-9ccb-40bb-983e-c0a094e42694",
   "metadata": {},
   "source": [
    "### Task Overview\n",
    "The problem we will solve is **predicting inhibitory activity of small molecules against certain protein-protein interactions (PPIs)**. \n",
    "\n",
    "Proteins rarely act alone as their functions tend to be regulated. Many molecular processes within a cell are dependents of PPIs. Some PPIs are involved in multiple aggregation-related diseases, such as Creutzfeldt–Jakob and Alzheimer's diseases. The discovery of novel molecules capable of inhibiting these processes can be of great importance for medicine.\n",
    "\n",
    "Small molecules are low molecular weight molecules that include lipids, monosaccharides, second messengers, other natural products and metabolites, as well as drugs and other xenobiotics. They can interact with receptors and regulate biological processes. \n",
    "\n",
    "The first thing we need is a data set with inhibitory activity values for real molecules. For this workshop, we will use datasets from [TIMBAL](https://pubmed.ncbi.nlm.nih.gov/23766369/) and [iPPIDB](https://pubmed.ncbi.nlm.nih.gov/33416858/). \n",
    "\n",
    "The inhibitory activity values are reported in **-log(IC50)**. IC50 means how much of a particular inhibitory drug is needed to inhibit a given biological process or biological component by 50\\%. It is measured in uM (micromolar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b66fe-11d7-4653-a190-939f2afc8ea3",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf179b4-3f3e-4ce1-98d0-fde1cd7a054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/carlosmr12/mlwk2023/master/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a1019-18ae-40a4-abaf-c7a0af245b07",
   "metadata": {},
   "source": [
    "### Loading general libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75fe54-e9fc-4a54-86a3-29c0b02864d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO-RELOAD EXTENSIONS\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# PLOTTING\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "# DATA MANIPULATION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PREVENTS GENERAL WARNINGS FROM SHOWING ON OUTPUT\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9035567e-6a89-45af-a1d8-eff0c9c6a989",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b3ecc-cab6-426b-8155-774976789ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/carlosmr12/mlwk2023/master/data/ppi_inhibitors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788f444-cc9f-4566-9873-40af2bd78777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"ppi_inhibitors.csv\") # Load data from file to a DataFrame structure\n",
    "print(df_data.shape) # .shape displays how the dataframe (matrix) looks like\n",
    "df_data.head() # .head() displays the first few items in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fe1bd-0b6d-4bbf-88f4-e308455c9acf",
   "metadata": {},
   "source": [
    "Data distribution based on data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11720a4-58d9-4158-b4d0-a37d1fa616b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['database'].value_counts())\n",
    "\n",
    "fig = px.pie(df_data, values=df_data['database'].value_counts().values, \n",
    "             names=df_data['database'].value_counts().index, \n",
    "             title='Data sources')\n",
    "\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5906b03-4b50-4293-a4d3-7f32f1167132",
   "metadata": {},
   "source": [
    "The molecules of the dataset are represented through **SMILES**. SMILES is a chemical notation that allows representation of a chemical structure.  They can be represented using simple vocabulary (atom and bond symbols), and few grammar rules: \n",
    "\n",
    "- ***2-Propanol would be “CC(O)C”***\n",
    "\n",
    "- ***2-Methylbutanal would be “CC(C)CC(=O)”.***\n",
    "\n",
    "Using a type of SMILES called ISOMERIC SMILES it is even possible to represent specific isotopism, configuration about double bonds, and chirality. \n",
    "\n",
    "Below you can see some examples of SMILES in our data:\n",
    "\n",
    "\n",
    "2D depiction of small molecules based on the SMILES format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c6fd3-5c79-4e38-90b1-264fae7a4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES FROM RDKIT\n",
    "from rdkit.Chem.Draw import IPythonConsole #RDKit drawing\n",
    "# A few settings to improve the quality of structures \n",
    "from rdkit.Chem import rdDepictor\n",
    "IPythonConsole.ipython_useSVG = True\n",
    "rdDepictor.SetPreferCoordGen(True)\n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2bef9-726c-468a-8435-fa5ea5e49cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.iloc[0]['SMILES'])\n",
    "m = Chem.MolFromSmiles(df_data.iloc[0]['SMILES'])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed08c42-42ba-439c-b884-105fc0968933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.iloc[-1]['SMILES'])\n",
    "m = Chem.MolFromSmiles(df_data.iloc[-1]['SMILES'])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c450f7-b4c1-4b03-a053-1bd2bc9cc3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.iloc[1]['SMILES'])\n",
    "m = Chem.MolFromSmiles(df_data.iloc[2]['SMILES'])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf2bc5-aa0c-470d-853e-b37ec29129f6",
   "metadata": {},
   "source": [
    "What does the distribution of our **target** variable look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c53b6-6912-44b0-a1bd-412351ea2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Experimental IC50 (log10)\"\n",
    "fig = px.histogram(df_data, x=target)\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f142ddc-9872-47e3-9183-fd4de344e07d",
   "metadata": {},
   "source": [
    "Why do we would preferably work with the target variable in a log scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dd25b-2219-4a47-a917-9ba65224feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['inverse_log'] = 10**df_data[target]\n",
    "fig = px.histogram(df_data, x='inverse_log')\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1860fe6-d337-4130-be99-e18d4cd21cda",
   "metadata": {},
   "source": [
    "It is also important to note that ours datasets covers, besides a long range of concentrations, also a range of values for diferent properties. This is also important to generate more generalised  models.\n",
    "\n",
    "An important concept here is **feature**. A feature is a property of the object you’re trying to predict. It can also be referred to as indepent variable, since it is a fixed property of the data point and it does not depend of others variables. These independent variables are essential, because the algorithms need characteristics of the data points as support for the learning process and predicting the labels (in our case inhibitory activity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b86a8-8c08-4e3f-bc17-d5ca0fc7d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"MolLogP\", \"Acceptor_Count\", \"Donor_Count\", \"NumRotatableBonds\",\n",
    "           \"RingCount\", \"MolWt\"]\n",
    "\n",
    "fig = make_subplots(rows=3, cols=2, start_cell=\"bottom-left\")\n",
    "\n",
    "nlines = 3\n",
    "ncolumns = 2\n",
    "\n",
    "for i in range(nlines):\n",
    "    for j in range(ncolumns):\n",
    "        fig.add_trace(go.Histogram(x=df_data[columns[i+j]], name=columns[i+j]), row=i+1, col=j+1)\n",
    "\n",
    "fig.update_yaxes(title=\"count\")\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c8a97-0380-4be3-83a1-46cdb209c5fc",
   "metadata": {},
   "source": [
    "### Regression (PPI inhibitory potency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8dac3-d93f-4d44-bd44-1df5f2b55cd4",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1005d-5fa4-4bd9-af20-5f6b185c0e18",
   "metadata": {},
   "source": [
    "The method **train_test_split()** used in the code below divides our data into two subsets: \n",
    "\n",
    "- One subset to *train* the model\n",
    "- A subset to evaluate or *test* how good your model is, which should\n",
    "    - Be large enough to yield statistically meaningful results\n",
    "    - Be representative of the data set as whole\n",
    "\n",
    "*(Never train on test data)*\n",
    "\n",
    "Splitting your dataset into training and test sets is very important and it is directly related to your models ability to *learn* patterns during the training step, which will ideally help to make it generalisable. Using the test set (a subset completely unseen during trainning) you can estimate the performance of the model when it is applied to new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d7ab0-5ea7-45af-b3e2-8079c4116bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "\n",
    "# What is the best TEST_SIZE?\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "columns = [\"MolLogP\", \"Acceptor_Count\", \"Donor_Count\", \"NumRotatableBonds\",\n",
    "           \"RingCount\", \"MolWt\"]\n",
    "X = df_data[columns]\n",
    "target = \"Experimental IC50 (log10)\"\n",
    "y = df_data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, start_cell=\"bottom-left\")\n",
    "\n",
    "fig.add_trace(go.Histogram(x=y_train, name='Training set'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=y_test, name='Test set'), row=1, col=2)\n",
    "fig.update_xaxes(title=target)\n",
    "fig.update_yaxes(title=\"count\")\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd5116-6eb9-4c08-93ab-70cf8b825b21",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a4b99-df34-46a5-a0eb-2194ab2b5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(max_depth=3)\n",
    "print(regressor.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807605b7-c703-4221-9c80-7add7b1a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = cross_val_predict(regressor, X_train, y_train, cv=5)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d4353-0565-4277-9bb8-14ef5929261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from tabulate import tabulate\n",
    "\n",
    "def regression_metrics(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    rmse_train = round(sqrt(mean_squared_error(y_train, y_pred_train)), 2)\n",
    "    pearsons_train = round(pearsonr(y_train, y_pred_train)[0], 2)\n",
    "    kendalls_train = round(kendalltau(y_train, y_pred_train)[0], 2)\n",
    "    spearmans_train = round(spearmanr(y_train, y_pred_train)[0], 2)\n",
    "    rmse_test = round(sqrt(mean_squared_error(y_test, y_pred_test)), 2)\n",
    "    pearsons_test = round(pearsonr(y_test, y_pred_test)[0], 2)\n",
    "    kendalls_test = round(kendalltau(y_test, y_pred_test)[0], 2)\n",
    "    spearmans_test = round(spearmanr(y_test, y_pred_test)[0], 2)\n",
    "\n",
    "    d = [ [\"RMSE\", rmse_train, rmse_test],\n",
    "         [\"Pearson's\", pearsons_train, pearsons_test],\n",
    "         [\"Kendall's\", kendalls_train, kendalls_test],\n",
    "         [\"Spearman's\", spearmans_train, spearmans_test]]\n",
    "\n",
    "    print(tabulate(d, headers=[\"Metric\", \"Training\", \"Test\"]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb458c3-18bd-4d3c-85e3-9918779b7b4e",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error**\n",
    "\n",
    "Root Mean Squared Error (RMSE) is a metric that tells us the average distance between the predicted values from the model and the actual values in the dataset (error). The lower the RMSE, the better a given model is able to “fit” a given dataset.\n",
    "\n",
    "$\\text{RMSE}(y, \\bar{y}) = \\sqrt{\\frac{\\sum_{i=1}^{N} (y_i - \\bar{y}_i)^2}{N}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0380b-6ac2-4141-b3f5-9bd16741b520",
   "metadata": {},
   "source": [
    "**Pearson's Correlation Coefficient**\n",
    "\n",
    "The Pearson correlation coefficient is probably the most widely used measure for **linear relationships** between two normal distributed variables and thus often just called \"correlation coefficient\". Generally, the Pearson coefficient is obtained via a **Least-Squares fit** and a value of ***1*** represents a ***perfect positive relation-ship***, ***-1*** a ***perfect negative relationship***, and ***0*** indicates the ***absence of a relationship*** between variables.\n",
    "\n",
    "$\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957e6c5-a92a-41ce-8503-fe7f3968b8e9",
   "metadata": {},
   "source": [
    "**Spearman's Correlation Coefficient**\n",
    "\n",
    "Spearman's *rho* can be understood as a rank-based version of Pearson's correlation coefficient, which can be used for variables that are not ***normal-distributed*** and have a ***non-linear relationship***. Also, its use is ***not only restricted to continuous data***, but can also be used in analyses of ordinal attributes.\n",
    "\n",
    "$\\rho = 1- {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019129b-f2e4-46e2-aad3-9848cd0f29dc",
   "metadata": {},
   "source": [
    "**Kendalls's Correlation Coefficient**\n",
    "\n",
    "Similar to the Pearson correlation coefficient, Kendall's *tau* measures the degree of **a monotone relationship between variables** ( the variables tend to move in the **same relative direction**, but **not necessarily at a constant rate**), and like Spearman's **rho**, it calculates the dependence between ranked variables, which makes is feasible for non-normal distributed data. Kendall tau can be calculated for continuous as well as ordinal data. Roughly speaking, Kendall's **tau** distinguishes itself from Spearman's **rho** by a \"harsher\" penalizations.\n",
    "\n",
    "$\\tau = \\frac{c-d}{c+d} = \\frac{S}{\n",
    "\t\\left(\n",
    "\t\\begin{matrix} \n",
    " \tn \\\\\n",
    " \t2\n",
    "\\end{matrix}\n",
    "\\right)}\n",
    "= \\frac{2S}{n(n-1)}$\n",
    "\n",
    "where $c$ is the number of *concordant pairs* and $d$ represents t=the number of *discordant pairs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101bdf4-0f92-4da3-9687-ce19cc8b05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a59eed-18d4-4b77-af86-c4cd59c43ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def regression_plots(y_train, pred_train, y_test, pred_test, target):\n",
    "    fig = make_subplots(rows=1, cols=2, start_cell=\"bottom-left\")\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=y_train, y=y_pred_train, name='Training set', \n",
    "                             mode=\"markers\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=y_test, y=y_pred_test, name='Test set', \n",
    "                             mode=\"markers\"), row=1, col=2)\n",
    "    \n",
    "    x = sm.add_constant(y_train)\n",
    "    p = sm.OLS(y_pred_train, x).fit().params\n",
    "    x = np.arange(y_train.min(), y_train.max())\n",
    "    y = p.const + p[target] * x\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, name='', mode=\"lines\", \n",
    "                             line=dict(dash='dash', color=\"black\"), \n",
    "                             showlegend=False), row=1, col=1)\n",
    "    \n",
    "    x = sm.add_constant(y_test)\n",
    "    p = sm.OLS(y_pred_test, x).fit().params\n",
    "    x = np.arange(y_test.min(), y_test.max())\n",
    "    y = p.const + p[target] * x\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, name='', mode=\"lines\", \n",
    "                             line=dict(dash='dash', color=\"black\"), \n",
    "                             showlegend=False), row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title=\"Actual\")\n",
    "    fig.update_yaxes(title=\"Prediction\")\n",
    "    fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed75b2e-27a4-4dcd-8b38-f8df429d1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_plots(y_train, y_pred_train, y_test, y_pred_test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5997b-ad0f-4e27-9575-7edcda3f39e3",
   "metadata": {},
   "source": [
    "Which feature is more important for the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff6808-33a3-40ce-83a5-f28b0f6a48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=regressor.feature_names_in_, y=regressor.feature_importances_, title=\"Feature importance\")\n",
    "fig.update_layout(yaxis_title=\"Importance score\", xaxis_title=\"Features\")\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490898c-148c-4448-835d-878f099d84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1 ,ncols=1, figsize=(9,6), dpi=300) # AVOID RUNNING THIS ON DECISION TREES WITH max_depth < 3 . IT COULD TAKE SOME TIME TO FINISH\n",
    "tree.plot_tree(regressor, feature_names=X_train.columns.tolist(), filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d557e-33cb-4fb4-adba-705ed5dee184",
   "metadata": {},
   "source": [
    "Impurity for regression in decision trees is generaly calculated in terms of \"variance reduction\".\n",
    "\n",
    "The decision tree algorithm tries the minimise the inpurity of the nodes in the tree.\n",
    "\n",
    "$ min\\frac{1}{N}\\sum_{i=1}^{i=N}(yi-\\bar{y})^2$\n",
    "\n",
    "Notice how the splits on the tree above try to \"group\" similar values (colours)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fe541-a420-496a-8184-5a8eb03cd977",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bbe1d-751a-43a0-a486-e5b437792452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "print(regressor.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe1a45-9ec9-4766-9247-055b60dd016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = cross_val_predict(regressor, X_train, y_train, cv=5)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6661ca-55f8-457f-a0d2-aa33ab7b1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c0f93-8587-4f60-a192-9352a7e4ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_plots(y_train, y_pred_train, y_test, y_pred_test, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492c5da-cf05-4dac-8e89-853ce69f83af",
   "metadata": {},
   "source": [
    "Which features were more important to the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e3000-9af6-4119-9f88-f738a1f42f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=regressor.feature_names_in_, y=regressor.feature_importances_, title=\"Feature importance\")\n",
    "fig.update_layout(yaxis_title=\"Importance score\", xaxis_title=\"Features\")\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2de76-9485-4afa-8f69-ee226220ec64",
   "metadata": {},
   "source": [
    "#### Exploring descriptors in the RDKit library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c163b-6041-4ee9-a0fd-ff76b6cc6ab8",
   "metadata": {},
   "source": [
    "Try and improve the performance on the predictive models using descriptors from the list below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94dd40-ae8f-4873-aff8-ba365257aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = [\"BalabanJ\",\"BertzCT\",\"Chi0\",\"Chi0n\",\"Chi0v\",\"Chi1\",\"Chi1n\",\n",
    "           # \"Chi1v\",\"Chi2n\",\"Chi2v\",\"Chi3n\",\"Chi3v\",\"Chi4n\",\"Chi4v\",\n",
    "           # \"HallKierAlpha\",\"Kappa1\",\"Kappa2\",\"Kappa3\",\"NHOHCount\",\n",
    "           # \"NOCount\",\"PEOE_VSA1\",\"PEOE_VSA10\",\"PEOE_VSA11\",\"PEOE_VSA12\",\n",
    "           \"PEOE_VSA13\",\"PEOE_VSA14\",\"PEOE_VSA2\",\"PEOE_VSA3\",\"PEOE_VSA4\",\n",
    "           \"PEOE_VSA5\",\"PEOE_VSA6\",\"PEOE_VSA7\",\"PEOE_VSA8\",\"PEOE_VSA9\",\n",
    "           # \"SMR_VSA1\",\"SMR_VSA10\",\"SMR_VSA2\",\"SMR_VSA3\",\"SMR_VSA4\",\"SMR_VSA5\",\n",
    "           # \"SMR_VSA6\",\"SMR_VSA7\",\"SMR_VSA8\",\"SMR_VSA9\",\"SlogP_VSA1\",\"SlogP_VSA10\",\n",
    "           # \"SlogP_VSA11\",\"SlogP_VSA12\",\"SlogP_VSA2\",\"SlogP_VSA3\",\"SlogP_VSA4\",\n",
    "           # \"SlogP_VSA5\",\"SlogP_VSA6\",\"SlogP_VSA7\",\"SlogP_VSA8\",\"SlogP_VSA9\",\n",
    "           # \"VSA_EState1\",\"VSA_EState10\",\"VSA_EState2\",\"VSA_EState3\",\n",
    "           # \"VSA_EState4\",\"VSA_EState5\",\"VSA_EState6\",\"VSA_EState7\",\n",
    "           # \"VSA_EState8\",\"VSA_EState9\",\"fr_Al_COO\",\"fr_Al_OH\",\"fr_Al_OH_noTert\",\n",
    "           # \"fr_ArN\",\"fr_Ar_COO\",\"fr_Ar_N\",\"fr_Ar_NH\",\"fr_Ar_OH\",\"fr_COO\",\"fr_COO2\",\n",
    "           # \"fr_C_O\",\"fr_C_O_noCOO\",\"fr_C_S\",\"fr_HOCCN\",\"fr_Imine\",\"fr_NH0\",\n",
    "           # \"fr_NH1\",\"fr_NH2\",\"fr_N_O\",\"fr_Ndealkylation1\",\"fr_Ndealkylation2\",\n",
    "           # \"fr_Nhpyrrole\",\"fr_SH\",\"fr_aldehyde\",\"fr_alkyl_carbamate\",\n",
    "           # \"fr_alkyl_halide\",\"fr_allylic_oxid\",\"fr_amide\",\"fr_amidine\",\"fr_aniline\",\n",
    "           # \"fr_aryl_methyl\",\"fr_azide\",\"fr_azo\",\"fr_barbitur\",\"fr_benzene\",\n",
    "           # \"fr_benzodiazepine\",\"fr_bicyclic\",\"fr_diazo\",\"fr_dihydropyridine\",\n",
    "           # \"fr_epoxide\",\"fr_ester\",\"fr_ether\",\"fr_furan\",\"fr_guanido\",\"fr_halogen\",\n",
    "           # \"fr_hdrzine\",\"fr_hdrzone\",\"fr_imidazole\",\"fr_imide\",\"fr_isocyan\",\n",
    "           # \"fr_isothiocyan\",\"fr_ketone\",\"fr_ketone_Topliss\",\"fr_lactam\",\"fr_lactone\",\n",
    "           # \"fr_methoxy\",\"fr_morpholine\",\"fr_nitrile\",\"fr_nitro\",\"fr_nitro_arom\",\n",
    "           # \"fr_nitro_arom_nonortho\",\"fr_nitroso\",\"fr_oxazole\",\"fr_oxime\",\n",
    "           # \"fr_para_hydroxylation\",\"fr_phenol\",\"fr_phenol_noOrthoHbond\",\"fr_phos_acid\",\n",
    "           # \"fr_phos_ester\",\"fr_piperdine\",\"fr_piperzine\",\"fr_priamide\",\n",
    "           # \"fr_prisulfonamd\",\"fr_pyridine\",\"fr_quatN\",\"fr_sulfide\",\"fr_sulfonamd\",\n",
    "           # \"fr_sulfone\",\"fr_term_acetylene\",\"fr_tetrazole\",\"fr_thiazole\",\"fr_thiocyan\",\n",
    "           \"fr_thiophene\",\"fr_unbrch_alkane\",\"fr_urea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29fc70-298d-45c0-80e7-fdf045525442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def get_descriptor(row):\n",
    "    molecule = Chem.MolFromSmiles(row['SMILES'])\n",
    "    return getattr(Descriptors, ds)(molecule)\n",
    "\n",
    "for ds in ds_list:\n",
    "    df_data[ds] = df_data.apply(get_descriptor, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bd6f9-02fc-4be2-87eb-65f8abc5469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73787565-9a42-458e-a564-96441bee8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best TEST_SIZE?\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "columns = [\"MolLogP\", \"Acceptor_Count\", \"Donor_Count\", \"NumRotatableBonds\",\n",
    "           \"RingCount\", \"MolWt\", \"PEOE_VSA13\", \"PEOE_VSA14\", \"PEOE_VSA2\",\n",
    "           \"PEOE_VSA3\", \"PEOE_VSA4\", \"PEOE_VSA5\", \"PEOE_VSA6\", \"PEOE_VSA7\",\n",
    "           \"PEOE_VSA8\", \"PEOE_VSA9\", \"BalabanJ\", \"BertzCT\", \"Chi0\", \n",
    "           \"Chi0n\", \"Chi0v\", \"Chi1\", \"Chi1n\", \"fr_thiophene\", \n",
    "           \"fr_unbrch_alkane\", \"fr_urea\"]\n",
    "\n",
    "target = \"Experimental IC50 (log10)\"\n",
    "\n",
    "X = df_data[columns]\n",
    "y = df_data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, start_cell=\"bottom-left\")\n",
    "\n",
    "fig.add_trace(go.Histogram(x=y_train, name='Training set'), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=y_test, name='Test set'), row=1, col=2)\n",
    "fig.update_xaxes(title=target)\n",
    "fig.update_yaxes(title=\"count\")\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c433d1-0717-4404-b3f8-b4ee2b5e848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb57efc-b23e-4c57-8fd5-3a8d278f175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = cross_val_predict(regressor, X_train, y_train, cv=5)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac3e64-3cd5-402b-9101-0ced4928c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics(y_train, y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d3773-d43c-41a3-bbae-d610b0041148",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_plots(y_train, y_pred_train, y_test, y_pred_test, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d63c1-df33-4a80-99c7-2c74a6dfa700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
